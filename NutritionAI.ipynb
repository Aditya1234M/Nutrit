{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya1234M/Nutrit/blob/main/NutritionAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stsIQS_O3wpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d298553-d92f-4411-cd47-3c8bd2df3f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "\n",
            "ğŸ”¥ GPU Detected!\n",
            "GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "âœ” Memory growth enabled.\n",
            "\n",
            "=== NVIDIA-SMI Output ===\n",
            "Sun Nov 23 15:41:42 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 â€” GPU setup (Colab T4)\n",
        "\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Check GPU using TensorFlow\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    print(\"\\nğŸ”¥ GPU Detected!\")\n",
        "    for gpu in gpus:\n",
        "        print(\"GPU:\", gpu)\n",
        "\n",
        "    # Enable safe memory growth\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"âœ” Memory growth enabled.\")\n",
        "    except Exception as e:\n",
        "        print(\"âš  Could not set memory growth:\", e)\n",
        "else:\n",
        "    print(\"\\nâŒ No GPU detected. Training will run on CPU (slower).\")\n",
        "\n",
        "# Check system GPU info (NVIDIA-SMI)\n",
        "print(\"\\n=== NVIDIA-SMI Output ===\")\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi\")\n",
        "print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 â€” Load Food-101 from TFDS\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# loads train and validation splits, as_supervised gives (image, label) pairs\n",
        "(food101_train_raw, food101_val_raw), food101_info = tfds.load(\n",
        "    \"food101\",\n",
        "    split=[\"train\", \"validation\"],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "num_food101_classes = food101_info.features[\"label\"].num_classes\n",
        "print(\"Food-101 classes:\", num_food101_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcJggvPJ6OvB",
        "outputId": "8786d94f-3974-4f8f-ca87-71d830358d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Food-101 classes: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"iamsouravbanerjee/indian-food-images-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhcV9GSm-TGJ",
        "outputId": "dd2abc2a-cb30-43a2-a320-a92e124c7662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'indian-food-images-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/indian-food-images-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Root:\", \"/kaggle/input/indian-food-images-dataset\")\n",
        "print(os.listdir(\"/kaggle/input/indian-food-images-dataset\"))\n",
        "\n",
        "# If inside there is a subfolder, list it too:\n",
        "sub1 = \"/kaggle/input/indian-food-images-dataset/\" + os.listdir(\"/kaggle/input/indian-food-images-dataset\")[0]\n",
        "print(\"Subfolder:\", sub1)\n",
        "print(os.listdir(sub1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ0SJq53qUmB",
        "outputId": "b1385003-c2d1-49b1-cf4c-79643eb247c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root: /kaggle/input/indian-food-images-dataset\n",
            "['Indian Food Images', 'List of Indian Foods.txt']\n",
            "Subfolder: /kaggle/input/indian-food-images-dataset/Indian Food Images\n",
            "['Indian Food Images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 â€” Indian dataset from KaggleHub path\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# IMPORTANT â€” Correct path from KaggleHub\n",
        "INDIAN_PATH = \"/kaggle/input/indian-food-images-dataset/Indian Food Images/Indian Food Images\"\n",
        "\n",
        "\n",
        "IMG_SIZE = 300\n",
        "BATCH = 16\n",
        "\n",
        "# Image generator\n",
        "indian_gen = ImageDataGenerator(\n",
        "    rescale=1/255.0,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Training generator\n",
        "indian_train_gen = indian_gen.flow_from_directory(\n",
        "    INDIAN_PATH,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH,\n",
        "    class_mode=\"sparse\",   # returns integer labels\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "# Validation generator\n",
        "indian_val_gen = indian_gen.flow_from_directory(\n",
        "    INDIAN_PATH,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH,\n",
        "    class_mode=\"sparse\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "print(\"Indian classes:\", indian_train_gen.num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDUKADXB9jbw",
        "outputId": "6ad14c96-affc-4df5-b412-d6afb796b3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3200 images belonging to 80 classes.\n",
            "Found 800 images belonging to 80 classes.\n",
            "Indian classes: 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 â€” preprocess food101 images (resize + normalize) and batch\n",
        "import tensorflow as tf\n",
        "\n",
        "def preprocess_food101(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "food101_train = food101_train_raw.map(preprocess_food101).shuffle(2048).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "food101_val   = food101_val_raw.map(preprocess_food101).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# quick sanity prints\n",
        "print(\"Food101 train batches:\", tf.data.experimental.cardinality(food101_train).numpy())\n",
        "print(\"Food101 val batches:\", tf.data.experimental.cardinality(food101_val).numpy())\n",
        "print(\"Indian train steps (batches):\", len(indian_train_gen))\n",
        "print(\"Indian val steps (batches):\", len(indian_val_gen))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MStdnP6D_Gkx",
        "outputId": "ea3206c7-4bc1-417f-febe-f2b6ca05d815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Food101 train batches: 4735\n",
            "Food101 val batches: 1579\n",
            "Indian train steps (batches): 200\n",
            "Indian val steps (batches): 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count classes from both datasets\n",
        "\n",
        "# Correct number of classes for Food-101\n",
        "num_food101_classes = food101_info.features[\"label\"].num_classes\n",
        "\n",
        "# Correct number of classes for Indian dataset\n",
        "num_indian_classes = indian_train_gen.num_classes   # this exists because you used ImageDataGenerator\n",
        "\n",
        "print(\"Food101 classes:\", num_food101_classes)\n",
        "print(\"Indian classes:\", num_indian_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_g2IRst_1CD",
        "outputId": "ce78ade8-35a5-492f-81c4-60b94c7aae85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Food101 classes: 101\n",
            "Indian classes: 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 â€” Build model with one shared backbone and two heads\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "base_model = tf.keras.applications.EfficientNetB4(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling=\"avg\",\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "base_model.trainable = False  # start with frozen backbone\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"input_image\")\n",
        "features = base_model(inputs, training=False)\n",
        "\n",
        "food101_head = layers.Dense(num_food101_classes, activation=\"softmax\", name=\"food101_head\")(features)\n",
        "indian_head  = layers.Dense(num_indian_classes, activation=\"softmax\", name=\"indian_head\")(features)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=[food101_head, indian_head], name=\"multihead_food_model\")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "gBjc9lCd_Zpb",
        "outputId": "4fc3346c-75be-419a-a7bd-f886c1f6ecc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"multihead_food_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"multihead_food_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_image         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ efficientnetb4      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)      â”‚ \u001b[38;5;34m17,673,823\u001b[0m â”‚ input_image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
              "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ food101_head        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)       â”‚    \u001b[38;5;34m181,093\u001b[0m â”‚ efficientnetb4[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
              "â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ indian_head (\u001b[38;5;33mDense\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)        â”‚    \u001b[38;5;34m143,440\u001b[0m â”‚ efficientnetb4[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_image         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ efficientnetb4      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,823</span> â”‚ input_image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ food101_head        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">181,093</span> â”‚ efficientnetb4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ indian_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)        â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">143,440</span> â”‚ efficientnetb4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,998,356\u001b[0m (68.66 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,998,356</span> (68.66 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m324,533\u001b[0m (1.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">324,533</span> (1.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,673,823\u001b[0m (67.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,823</span> (67.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 â€” Compile the model\n",
        "\n",
        "losses = {\n",
        "    \"food101_head\": \"sparse_categorical_crossentropy\",\n",
        "    \"indian_head\": \"sparse_categorical_crossentropy\",\n",
        "}\n",
        "\n",
        "metrics = {\n",
        "    \"food101_head\": [\"accuracy\"],\n",
        "    \"indian_head\": [\"accuracy\"],\n",
        "}\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=losses,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfxP8HJ7Bosv",
        "outputId": "57f4c379-b3d4-48f4-8816-bd31dd572af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Indian DirectoryIterator â†’ tf.data.Dataset\n",
        "\n",
        "def indian_gen_to_dataset(gen):\n",
        "    output_types = (tf.float32, tf.float32)       # (images, labels)\n",
        "    output_shapes = (tf.TensorShape([None, IMG_SIZE, IMG_SIZE, 3]),\n",
        "                     tf.TensorShape([None]))      # labels shape\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        lambda: gen,\n",
        "        output_types=output_types,\n",
        "        output_shapes=output_shapes\n",
        "    )\n"
      ],
      "metadata": {
        "id": "5m7SSCvPCdeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indian_train_ds = indian_gen_to_dataset(indian_train_gen)\n",
        "indian_val_ds   = indian_gen_to_dataset(indian_val_gen)\n"
      ],
      "metadata": {
        "id": "MsmaehlmChPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_food101(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "JMuXzaS9DqIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap both datasets in the format the multi-head model expects\n",
        "\n",
        "def wrap_indian(images, labels):\n",
        "    return images, {\n",
        "        \"food101_head\": tf.zeros_like(labels),\n",
        "        \"indian_head\": labels\n",
        "    }\n",
        "\n",
        "def wrap_food101(image, label):\n",
        "    return image, {\n",
        "        \"food101_head\": label,\n",
        "        \"indian_head\": tf.zeros_like(label)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Wp8hoXfZCoRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Food-101 datasets (already tf.data)\n",
        "food101_train = (\n",
        "    food101_train_raw\n",
        "    .map(preprocess_food101, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .map(wrap_food101, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(BATCH)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "food101_val = (\n",
        "    food101_val_raw\n",
        "    .map(preprocess_food101, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .map(wrap_food101, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(BATCH)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "\n",
        "# Indian datasets converted from generator\n",
        "indian_train = indian_train_ds.map(wrap_indian).prefetch(tf.data.AUTOTUNE)\n",
        "indian_val   = indian_val_ds.map(wrap_indian).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "Ken8xxiLCr6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 â€” Loss weights\n",
        "\n",
        "loss_weights_food101 = {\n",
        "    \"food101_head\": 1.0,\n",
        "    \"indian_head\": 0.0\n",
        "}\n",
        "\n",
        "loss_weights_indian = {\n",
        "    \"food101_head\": 0.0,\n",
        "    \"indian_head\": 1.0\n",
        "}\n"
      ],
      "metadata": {
        "id": "UyJ7rUkeCzoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in food101_train.take(1):\n",
        "    print(\"Food101 shape:\", images.shape)\n",
        "\n",
        "for images, labels in indian_train.take(1):\n",
        "    print(\"Indian shape:\", images.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xqPlU2ND-xj",
        "outputId": "f2c9b766-d298-4811-de48-3d12b943906a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Food101 shape: (16, 300, 300, 3)\n",
            "Indian shape: (16, 300, 300, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in food101_val.take(2):\n",
        "    img, lbl = batch\n",
        "    print(\"Food101 val batch OK:\", img.shape)\n",
        "    break\n",
        "\n",
        "for batch in indian_val.take(2):\n",
        "    img, lbl = batch\n",
        "    print(\"Indian val batch OK:\", img.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lodAu-eJQo2h",
        "outputId": "747c5c14-2888-46ce-d03d-b17700e9600b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Food101 val batch OK: (16, 300, 300, 3)\n",
            "Indian val batch OK: (16, 300, 300, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ğŸ”’ STAGE 1: FREEZE BACKBONE\n",
        "# ============================\n",
        "\n",
        "# Freeze ALL layers of EfficientNetB4\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(\"Backbone frozen. Training only final classification heads.\")\n",
        "\n",
        "# Compile with a higher LR for head training\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss={\n",
        "        \"food101_head\": \"sparse_categorical_crossentropy\",\n",
        "        \"indian_head\": \"sparse_categorical_crossentropy\"\n",
        "    },\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVOtE1m7wQ5X",
        "outputId": "d93b085d-46d8-402d-aa29-e81ee7c1ad62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone frozen. Training only final classification heads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze backbone\n",
        "base_model.trainable = False\n",
        "\n",
        "# Compile for stage 1\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss={\n",
        "        \"food101_head\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        \"indian_head\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    },\n",
        "    metrics={\n",
        "        \"food101_head\": \"accuracy\",\n",
        "        \"indian_head\": \"accuracy\"\n",
        "    }\n",
        ")\n",
        "\n",
        "EPOCHS_STAGE1 = 3\n",
        "\n",
        "history1 = model.fit(\n",
        "    food101_train.zip(indian_train),\n",
        "    validation_data=food101_val.zip(indian_val),\n",
        "    epochs=EPOCHS_STAGE1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "vzWXEvNSywZ6",
        "outputId": "905ad097-023a-4edb-c0c6-9200e6702d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "    244/Unknown \u001b[1m86s\u001b[0m 122ms/step - food101_head_accuracy: 0.9496 - food101_head_loss: 0.6500 - indian_head_accuracy: 0.0081 - indian_head_loss: 4.5105 - loss: 5.1605"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3666002266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mEPOCHS_STAGE1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m history1 = model.fit(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mfood101_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindian_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfood101_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindian_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 â€” Correct multi-task training loop\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "# steps per epoch for each dataset\n",
        "steps_food101 = food101_train_raw.cardinality().numpy() // BATCH\n",
        "steps_indian  = indian_train_gen.samples // BATCH\n",
        "\n",
        "# choose the larger one\n",
        "steps_per_epoch = 200\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    food101_iter = iter(food101_train)\n",
        "    indian_iter = iter(indian_train)\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "\n",
        "        # ---- FOOD101 batch ----\n",
        "        try:\n",
        "            x_f, y_f = next(food101_iter)\n",
        "            model.train_on_batch(x_f, y_f)\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "        # ---- INDIAN batch ----\n",
        "        try:\n",
        "            x_i, y_i = next(indian_iter)\n",
        "            model.train_on_batch(x_i, y_i)\n",
        "        except StopIteration:\n",
        "            pass\n",
        "        if step % 20 == 0:\n",
        "            print(f\"Step {step}/{steps_per_epoch}\")\n",
        "\n",
        "    # ---- Validation ----\n",
        "    print(\"Validating...\")\n",
        "\n",
        "    val_food = model.evaluate(food101_val.take(50), verbose=0)\n",
        "    val_ind  = model.evaluate(indian_val.take(5), verbose=0)\n",
        "\n",
        "    print(f\"Food101 Val Loss/Acc: {val_food}\")\n",
        "    print(f\"Indian Val Loss/Acc : {val_ind}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b9f63f-989d-4fc5-c2b3-a3697b58f070",
        "id": "RYyUndW6Mly2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0/200\n",
            "Step 20/200\n",
            "Step 40/200\n",
            "Step 60/200\n",
            "Step 80/200\n",
            "Step 100/200\n",
            "Step 120/200\n",
            "Step 140/200\n",
            "Step 160/200\n",
            "Step 180/200\n",
            "Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Food101 Val Loss/Acc: [6.061058521270752, 5.437210559844971, 0.6238466501235962, 0.008750000037252903, 1.0]\n",
            "Indian Val Loss/Acc : [5.769024848937988, 0.6395354866981506, 5.129488945007324, 1.0, 0.0]\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0/200\n",
            "Step 20/200\n",
            "Step 40/200\n",
            "Step 60/200\n",
            "Step 80/200\n",
            "Step 100/200\n",
            "Step 120/200\n",
            "Step 140/200\n",
            "Step 160/200\n",
            "Step 180/200\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [6.0302042961120605, 5.401789665222168, 0.6284130215644836, 0.008750000037252903, 1.0]\n",
            "Indian Val Loss/Acc : [5.586277961730957, 0.6384649276733398, 4.947813510894775, 1.0, 0.03750000149011612]\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0/200\n",
            "Step 20/200\n",
            "Step 40/200\n",
            "Step 60/200\n",
            "Step 80/200\n",
            "Step 100/200\n",
            "Step 120/200\n",
            "Step 140/200\n",
            "Step 160/200\n",
            "Step 180/200\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [6.015382289886475, 5.4053778648376465, 0.6100038290023804, 0.008750000037252903, 1.0]\n",
            "Indian Val Loss/Acc : [5.869945049285889, 0.6110502481460571, 5.258894920349121, 1.0, 0.0]\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0/200\n",
            "Step 20/200\n",
            "Step 40/200\n",
            "Step 60/200\n",
            "Step 80/200\n",
            "Step 100/200\n",
            "Step 120/200\n",
            "Step 140/200\n",
            "Step 160/200\n",
            "Step 180/200\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [5.931065082550049, 5.361526966094971, 0.569537341594696, 0.008750000037252903, 1.0]\n",
            "Indian Val Loss/Acc : [5.82845401763916, 0.6424292325973511, 5.186025142669678, 1.0, 0.012500000186264515]\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0/200\n",
            "Step 20/200\n",
            "Step 40/200\n",
            "Step 60/200\n",
            "Step 80/200\n",
            "Step 100/200\n",
            "Step 120/200\n",
            "Step 140/200\n",
            "Step 160/200\n",
            "Step 180/200\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [5.9434099197387695, 5.360986709594727, 0.5824229121208191, 0.008750000037252903, 1.0]\n",
            "Indian Val Loss/Acc : [5.722454071044922, 0.6446283459663391, 5.077825546264648, 1.0, 0.012500000186264515]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"multihead_food_model.h5\")\n",
        "print(\"Saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38cIlLHJYgEq",
        "outputId": "227ba3d8-b8cc-4b6e-e639-7bd932cb21e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze last 50 layers\n",
        "for layer in base_model.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=losses,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "print(\"Fine-tuning startedâ€¦\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqbcoo8UYnAJ",
        "outputId": "b69b98d9-673a-44a2-852b-30c8fdfc0c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning startedâ€¦\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "# steps per epoch for each dataset\n",
        "steps_food101 = food101_train_raw.cardinality().numpy() // BATCH\n",
        "steps_indian  = indian_train_gen.samples // BATCH\n",
        "\n",
        "# choose the larger one\n",
        "steps_per_epoch = 1000\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    food101_iter = iter(food101_train)\n",
        "    indian_iter = iter(indian_train)\n",
        "\n",
        "    for step in range(steps_per_epoch):\n",
        "\n",
        "        # ---- FOOD101 batch ----\n",
        "        try:\n",
        "            x_f, y_f = next(food101_iter)\n",
        "            model.train_on_batch(x_f, y_f)\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "        # ---- INDIAN batch ----\n",
        "        try:\n",
        "            x_i, y_i = next(indian_iter)\n",
        "            model.train_on_batch(x_i, y_i)\n",
        "        except StopIteration:\n",
        "            pass\n",
        "        if step % 20 == 0:\n",
        "            print(f\"Step {step}/{steps_per_epoch}\")\n",
        "\n",
        "    # ---- Validation ----\n",
        "    print(\"Validating...\")\n",
        "\n",
        "    val_food = model.evaluate(food101_val.take(50), verbose=0)\n",
        "    val_ind  = model.evaluate(indian_val.take(5), verbose=0)\n",
        "\n",
        "    print(f\"Food101 Val Loss/Acc: {val_food}\")\n",
        "    print(f\"Indian Val Loss/Acc : {val_ind}\")\n"
      ],
      "metadata": {
        "id": "HVwu-g1iYrEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf6dffa-2a53-4ede-af49-9f8c93858012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0/1000\n",
            "Step 20/1000\n",
            "Step 40/1000\n",
            "Step 60/1000\n",
            "Step 80/1000\n",
            "Step 100/1000\n",
            "Step 120/1000\n",
            "Step 140/1000\n",
            "Step 160/1000\n",
            "Step 180/1000\n",
            "Step 200/1000\n",
            "Step 220/1000\n",
            "Step 240/1000\n",
            "Step 260/1000\n",
            "Step 280/1000\n",
            "Step 300/1000\n",
            "Step 320/1000\n",
            "Step 340/1000\n",
            "Step 360/1000\n",
            "Step 380/1000\n",
            "Step 400/1000\n",
            "Step 420/1000\n",
            "Step 440/1000\n",
            "Step 460/1000\n",
            "Step 480/1000\n",
            "Step 500/1000\n",
            "Step 520/1000\n",
            "Step 540/1000\n",
            "Step 560/1000\n",
            "Step 580/1000\n",
            "Step 600/1000\n",
            "Step 620/1000\n",
            "Step 640/1000\n",
            "Step 660/1000\n",
            "Step 680/1000\n",
            "Step 700/1000\n",
            "Step 720/1000\n",
            "Step 740/1000\n",
            "Step 760/1000\n",
            "Step 780/1000\n",
            "Step 800/1000\n",
            "Step 820/1000\n",
            "Step 840/1000\n",
            "Step 860/1000\n",
            "Step 880/1000\n",
            "Step 900/1000\n",
            "Step 920/1000\n",
            "Step 940/1000\n",
            "Step 960/1000\n",
            "Step 980/1000\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [6.632874011993408, 5.690617561340332, 0.942257285118103, 0.008750000037252903, 0.9987499713897705]\n",
            "Indian Val Loss/Acc : [5.182485580444336, 0.3831041753292084, 4.799381732940674, 0.987500011920929, 0.0]\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0/1000\n",
            "Step 20/1000\n",
            "Step 40/1000\n",
            "Step 60/1000\n",
            "Step 80/1000\n",
            "Step 100/1000\n",
            "Step 120/1000\n",
            "Step 140/1000\n",
            "Step 160/1000\n",
            "Step 180/1000\n",
            "Step 200/1000\n",
            "Step 220/1000\n",
            "Step 240/1000\n",
            "Step 260/1000\n",
            "Step 280/1000\n",
            "Step 300/1000\n",
            "Step 320/1000\n",
            "Step 340/1000\n",
            "Step 360/1000\n",
            "Step 380/1000\n",
            "Step 400/1000\n",
            "Step 420/1000\n",
            "Step 440/1000\n",
            "Step 460/1000\n",
            "Step 480/1000\n",
            "Step 500/1000\n",
            "Step 520/1000\n",
            "Step 540/1000\n",
            "Step 560/1000\n",
            "Step 580/1000\n",
            "Step 600/1000\n",
            "Step 620/1000\n",
            "Step 640/1000\n",
            "Step 660/1000\n",
            "Step 680/1000\n",
            "Step 700/1000\n",
            "Step 720/1000\n",
            "Step 740/1000\n",
            "Step 760/1000\n",
            "Step 780/1000\n",
            "Step 800/1000\n",
            "Step 820/1000\n",
            "Step 840/1000\n",
            "Step 860/1000\n",
            "Step 880/1000\n",
            "Step 900/1000\n",
            "Step 920/1000\n",
            "Step 940/1000\n",
            "Step 960/1000\n",
            "Step 980/1000\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [6.8416852951049805, 5.881425857543945, 0.9602592587471008, 0.008750000037252903, 0.9987499713897705]\n",
            "Indian Val Loss/Acc : [5.048081874847412, 0.29896020889282227, 4.74912166595459, 1.0, 0.012500000186264515]\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0/1000\n",
            "Step 20/1000\n",
            "Step 40/1000\n",
            "Step 60/1000\n",
            "Step 80/1000\n",
            "Step 100/1000\n",
            "Step 120/1000\n",
            "Step 140/1000\n",
            "Step 160/1000\n",
            "Step 180/1000\n",
            "Step 200/1000\n",
            "Step 220/1000\n",
            "Step 240/1000\n",
            "Step 260/1000\n",
            "Step 280/1000\n",
            "Step 300/1000\n",
            "Step 320/1000\n",
            "Step 340/1000\n",
            "Step 360/1000\n",
            "Step 380/1000\n",
            "Step 400/1000\n",
            "Step 420/1000\n",
            "Step 440/1000\n",
            "Step 460/1000\n",
            "Step 480/1000\n",
            "Step 500/1000\n",
            "Step 520/1000\n",
            "Step 540/1000\n",
            "Step 560/1000\n",
            "Step 580/1000\n",
            "Step 600/1000\n",
            "Step 620/1000\n",
            "Step 640/1000\n",
            "Step 660/1000\n",
            "Step 680/1000\n",
            "Step 700/1000\n",
            "Step 720/1000\n",
            "Step 740/1000\n",
            "Step 760/1000\n",
            "Step 780/1000\n",
            "Step 800/1000\n",
            "Step 820/1000\n",
            "Step 840/1000\n",
            "Step 860/1000\n",
            "Step 880/1000\n",
            "Step 900/1000\n",
            "Step 920/1000\n",
            "Step 940/1000\n",
            "Step 960/1000\n",
            "Step 980/1000\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [7.457646369934082, 6.134893417358398, 1.322756290435791, 0.008750000037252903, 0.9975000023841858]\n",
            "Indian Val Loss/Acc : [4.788704872131348, 0.2965294122695923, 4.492176055908203, 0.987500011920929, 0.02500000037252903]\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0/1000\n",
            "Step 20/1000\n",
            "Step 40/1000\n",
            "Step 60/1000\n",
            "Step 80/1000\n",
            "Step 100/1000\n",
            "Step 120/1000\n",
            "Step 140/1000\n",
            "Step 160/1000\n",
            "Step 180/1000\n",
            "Step 200/1000\n",
            "Step 220/1000\n",
            "Step 240/1000\n",
            "Step 260/1000\n",
            "Step 280/1000\n",
            "Step 300/1000\n",
            "Step 320/1000\n",
            "Step 340/1000\n",
            "Step 360/1000\n",
            "Step 380/1000\n",
            "Step 400/1000\n",
            "Step 420/1000\n",
            "Step 440/1000\n",
            "Step 460/1000\n",
            "Step 480/1000\n",
            "Step 500/1000\n",
            "Step 520/1000\n",
            "Step 540/1000\n",
            "Step 560/1000\n",
            "Step 580/1000\n",
            "Step 600/1000\n",
            "Step 620/1000\n",
            "Step 640/1000\n",
            "Step 660/1000\n",
            "Step 680/1000\n",
            "Step 700/1000\n",
            "Step 720/1000\n",
            "Step 740/1000\n",
            "Step 760/1000\n",
            "Step 780/1000\n",
            "Step 800/1000\n",
            "Step 820/1000\n",
            "Step 840/1000\n",
            "Step 860/1000\n",
            "Step 880/1000\n",
            "Step 900/1000\n",
            "Step 920/1000\n",
            "Step 940/1000\n",
            "Step 960/1000\n",
            "Step 980/1000\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [7.542391300201416, 6.235045909881592, 1.3073452711105347, 0.008750000037252903, 0.9987499713897705]\n",
            "Indian Val Loss/Acc : [5.139697074890137, 0.45831966400146484, 4.681377410888672, 0.9750000238418579, 0.012500000186264515]\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0/1000\n",
            "Step 20/1000\n",
            "Step 40/1000\n",
            "Step 60/1000\n",
            "Step 80/1000\n",
            "Step 100/1000\n",
            "Step 120/1000\n",
            "Step 140/1000\n",
            "Step 160/1000\n",
            "Step 180/1000\n",
            "Step 200/1000\n",
            "Step 220/1000\n",
            "Step 240/1000\n",
            "Step 260/1000\n",
            "Step 280/1000\n",
            "Step 300/1000\n",
            "Step 320/1000\n",
            "Step 340/1000\n",
            "Step 360/1000\n",
            "Step 380/1000\n",
            "Step 400/1000\n",
            "Step 420/1000\n",
            "Step 440/1000\n",
            "Step 460/1000\n",
            "Step 480/1000\n",
            "Step 500/1000\n",
            "Step 520/1000\n",
            "Step 540/1000\n",
            "Step 560/1000\n",
            "Step 580/1000\n",
            "Step 600/1000\n",
            "Step 620/1000\n",
            "Step 640/1000\n",
            "Step 660/1000\n",
            "Step 680/1000\n",
            "Step 700/1000\n",
            "Step 720/1000\n",
            "Step 740/1000\n",
            "Step 760/1000\n",
            "Step 780/1000\n",
            "Step 800/1000\n",
            "Step 820/1000\n",
            "Step 840/1000\n",
            "Step 860/1000\n",
            "Step 880/1000\n",
            "Step 900/1000\n",
            "Step 920/1000\n",
            "Step 940/1000\n",
            "Step 960/1000\n",
            "Step 980/1000\n",
            "Validating...\n",
            "Food101 Val Loss/Acc: [7.405229568481445, 6.103863716125488, 1.301365613937378, 0.008750000037252903, 1.0]\n",
            "Indian Val Loss/Acc : [4.882623195648193, 0.3389711380004883, 4.543652534484863, 0.987500011920929, 0.012500000186264515]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"multihead_final.h5\")\n"
      ],
      "metadata": {
        "id": "X8lqziTda-67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c1d404-c425-4eea-bb97-66af5734433b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nğŸ”“ Unfreezing TOP 50 EfficientNet layers...\")\n",
        "\n",
        "# Unfreeze only the top 50 layers\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in base_model.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "print(\"Trainable layers:\", sum([layer.trainable for layer in base_model.layers]))\n",
        "\n",
        "# Very small LR for fine-tuning\n",
        "fine_tune_lr = 3e-5\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(fine_tune_lr),\n",
        "    loss={\n",
        "        \"food101_head\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        \"indian_head\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    },\n",
        "    metrics={\n",
        "        \"food101_head\": \"accuracy\",\n",
        "        \"indian_head\": \"accuracy\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Reduce dataset steps (FAST!)\n",
        "steps_per_epoch = 200     # instead of 4700\n",
        "\n",
        "EPOCHS_FT = 2            # instead of 6\n",
        "\n",
        "history3 = model.fit(\n",
        "    food101_train.zip(indian_train),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=EPOCHS_FT\n",
        ")\n",
        "\n",
        "print(\"ğŸ‰ Fast fine-tuning done!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLJz9djNNPmI",
        "outputId": "106266ea-99e0-410b-e044-1c31670eb267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”“ Unfreezing TOP 50 EfficientNet layers...\n",
            "Trainable layers: 50\n",
            "Epoch 1/2\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 115ms/step - food101_head_accuracy: 0.8669 - food101_head_loss: 1.4099 - indian_head_accuracy: 0.0185 - indian_head_loss: 4.4831 - loss: 5.8930\n",
            "Epoch 2/2\n",
            "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 117ms/step - food101_head_accuracy: 1.0000 - food101_head_loss: 0.0241 - indian_head_accuracy: 0.0214 - indian_head_loss: 4.3687 - loss: 4.3928\n",
            "ğŸ‰ Fast fine-tuning done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nğŸ”“ Unfreezing ALL EfficientNet layers for final fine-tuning...\")\n",
        "\n",
        "# Unfreeze whole backbone\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "print(\"Trainable layers:\", sum([layer.trainable for layer in base_model.layers]))\n",
        "\n",
        "# --- Very small LR ---\n",
        "fine_tune_lr = 1e-5\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(fine_tune_lr),\n",
        "    loss={\n",
        "        \"food101_head\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        \"indian_head\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    },\n",
        "    metrics={\n",
        "        \"food101_head\": \"accuracy\",\n",
        "        \"indian_head\": \"accuracy\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Proper steps based on dataset size\n",
        "steps_food101 = food101_train_raw.cardinality().numpy() // BATCH\n",
        "steps_indian  = indian_train_gen.samples // BATCH\n",
        "\n",
        "steps_per_epoch = steps_food101    # Food101 dominates\n",
        "\n",
        "EPOCHS_FT = 2\n",
        "\n",
        "history3 = model.fit(\n",
        "    food101_train.zip(indian_train),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=EPOCHS_FT\n",
        ")\n",
        "\n",
        "print(\"ğŸ‰ Fine-tuning complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tES20OwfBsu4",
        "outputId": "3719109e-ca85-4f91-a2b5-f356a9185aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”“ Unfreezing ALL EfficientNet layers for final fine-tuning...\n",
            "Trainable layers: 476\n",
            "Epoch 1/2\n",
            "\u001b[1m4734/4734\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1665s\u001b[0m 319ms/step - food101_head_accuracy: 0.9916 - food101_head_loss: 0.2058 - indian_head_accuracy: 0.1967 - indian_head_loss: 3.5765 - loss: 3.7823\n",
            "Epoch 2/2\n",
            "\u001b[1m2640/4734\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m11:06\u001b[0m 318ms/step - food101_head_accuracy: 1.0000 - food101_head_loss: 0.0096 - indian_head_accuracy: 0.8593 - indian_head_loss: 0.8892 - loss: 0.8988"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whAsh5nmL3ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nğŸ”“ Unfreezing ALL EfficientNet layers for final fine-tuning...\")\n",
        "\n",
        "# Unfreeze whole backbone\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "print(\"Trainable layers:\", sum([layer.trainable for layer in base_model.layers]))\n",
        "\n",
        "# --- Very small LR ---\n",
        "fine_tune_lr = 1e-5\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(fine_tune_lr),\n",
        "    loss={\n",
        "        \"food101_head\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        \"indian_head\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    },\n",
        "    metrics={\n",
        "        \"food101_head\": \"accuracy\",\n",
        "        \"indian_head\": \"accuracy\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Proper steps based on dataset size\n",
        "steps_food101 = food101_train_raw.cardinality().numpy() // BATCH\n",
        "steps_indian  = indian_train_gen.samples // BATCH\n",
        "\n",
        "steps_per_epoch = steps_food101    # Food101 dominates\n",
        "\n",
        "EPOCHS_FT = 6\n",
        "\n",
        "history3 = model.fit(\n",
        "    food101_train.zip(indian_train),\n",
        "    validation_data=food101_val.zip(indian_val),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=EPOCHS_FT\n",
        ")\n",
        "\n",
        "print(\"ğŸ‰ Fine-tuning complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5971ad9-b2ca-4d41-f773-3244f255ba7e",
        "id": "A7G_Qf3ZL3wd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”“ Unfreezing ALL EfficientNet layers for final fine-tuning...\n",
            "Trainable layers: 476\n",
            "Epoch 1/6\n",
            "\u001b[1m4734/4734\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - food101_head_accuracy: 0.9799 - food101_head_loss: 0.3038 - indian_head_accuracy: 0.2178 - indian_head_loss: 3.4054 - loss: 3.7092"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"multihead_final.h5\")\n",
        "print(\"Model saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSxQiXZjeG6j",
        "outputId": "1c28fbb0-1bd8-4e00-a352-7e81c10e7fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/multi_food_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC3VcW5tl_LD",
        "outputId": "eabbed24-d802-4d81-f508-4ced11c72c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}